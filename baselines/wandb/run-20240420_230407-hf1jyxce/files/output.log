
[DEBUG] LOADED TOKENIZER
Traceback (most recent call last):
  File "/nas/ucb/tutrinh/selfexplainingai/baselines/llm_baseline.py", line 295, in <module>
    main()
  File "/nas/ucb/tutrinh/selfexplainingai/baselines/llm_baseline.py", line 229, in main
    training_set, fit_prompts, fit_responses = tokenize_dataset(training_data, wandb.config["model"], True, max_input_token_length = wandb.config["max_input_token_length"])
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas/ucb/tutrinh/selfexplainingai/baselines/llm_baseline.py", line 94, in tokenize_dataset
    max_length, valid_prompt_indices = _find_max_length(tokenizer, df, max_input_token_length)
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas/ucb/tutrinh/selfexplainingai/baselines/llm_baseline.py", line 136, in _find_max_length
    prompt_tokenization = tokenizer(df["prompt"].tolist(), truncation = True)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas/ucb/tutrinh/anaconda3/envs/chai/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2803, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas/ucb/tutrinh/anaconda3/envs/chai/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2861, in _call_one
    raise ValueError(
ValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).
None